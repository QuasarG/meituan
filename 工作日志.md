# 工作日志

## 4.11

1. rider_information去重
2. `merge.py` 实现合并三个数据表，每一行代表一个骑手在某一天的表现，以及这一天的天气状态（189816行）
   合并为 `merged_data.csv`
3. 重新运行了骑手聚类，结果为 `cluster_analysis_rider.csv`，此时最佳聚类数为3
4. 对天气数据进行预处理，因为对同一天出现了不同的数据，选取该天对应字段出现最多的为正确值
   使用 `duplicate_weather.py`进行了去重，输出为 `data\processed_data\duplicated_weather.csv`
5. 删除 `merged_data.csv`，重新运行 `merge.py`，输出为 `merged_data.csv`
6. 去除 `merge.py`中对于只在某一个表中出现的处理进行优化。想要的仅仅是找出两个表格里面是否有单独出现的骑手，也就是无法匹配的情况，这种数据无法进行分析，需要删除，但是结果显示只在骑手信息表中存在的骑手ID数量: 0，只在行为数据表中存在的骑手ID数量: 0，是不是说所有的骑手在两个表格里都是相互匹配的？只是有空值？这一段处理有些模糊，进行修改，如果确实是对应好了的，那么生成info_only和behavior_only两个文件就没必要了
7. 生成消息显示不重复骑手id数量3276，直接分别读取两个骑手原始数据检测不重复id数量为3276，数据吻合。
8. 检查合并生成的数据是否吻合：
   骑手信息表中不重复的骑手ID数量: 3276
   行为数据表中不重复的骑手ID数量: 3276
   合并数据表中不重复的骑手ID数量: 3276
   吻合。

---

## 4.12

1. 发现rider_information表有3604行，中存在重复的骑手id，需要进行合并
   成功去重，目前rider_information表中不重复的骑手ID数量: 3276
2. 好像忘了把新的天气替换原来的天气了，需要把duplicated_weather.csv替换掉原来的weather.csv
   重新合并后，出现57587条记录
3. 突然想到rider_daily_behavior表中也有有可能出现同一id同一天出现不同状况的可能，进行一下检查 `deduplicated_rider_behavior.py`
   检查结果显示无重复，但是不放心，需要进行验证（57587 = 骑手id数 * 每个人工作天数）
   去重后数据总行数: 57587
   不重复骑手ID数量: 3276
   所有骑手工作天数总和: 57587
   验证结果: 通过
4. 重新合并，目前merged_data.csv中共有57587行，数据吻合，按id排序，每一个id的日期是升序连续的
5. 进行缺失值统计，缺失超过50%的列不做考虑（单独拿出来？）

---

## 4.15

1. 缺失值的处理方式？
   目前已知：
    | 字段名称 | 缺失比例 |
    | ---- | ---- |
   |rider_id      |                  0.00|
   |gender        |                  0.00|
   |age            |                 0.00|
   |born_province   |                0.00|
   |regist_date       |              0.00|
   |education_description   |       60.36|
   |marriage_status_description   | 60.82|
   |children_num_description    |   61.60|
   |dt                         |     0.00|
   |cnt_waybill                |     0.00|
   |cnt_waybill_reject         |     0.00|
   |cnt_waybill_ontime         |     0.00|
   |interval_waybill           |     0.04|
   |interval_work              |     0.04|
   |weight_weather_level       |     0.00|
   |temperature_range          |     0.00|
   |real_feel_range            |     0.00|
   |humidity_range             |     0.00|
   |wind_grade_range           |     0.00|
   |wind_range                 |     0.00|
   |rain_intensity_range       |     0.00|

2. 明确下一步：
对缺失值做相关性分析，若相关性不高，缺失60%以上的变量直接删除；
若特征重要，可保留缺失状态，新增二值特征（如 is_education_missing）标记缺失情况，而非简单填充
暂时去除这几个高缺失比例变量
好像没有确定下来固定的某个目标变量，所以先不管，具体处理方式参照下面的：

**原则：保留信息，避免过早决策**
暂不删除或填充：
若无法通过业务逻辑直接判断这些特征的重要性（如 education_description、marriage_status_description），建议暂时保留原始缺失状态，避免因盲目填充或删除损失潜在有用信息。
示例代码：
```python
# 记录缺失率高的特征，暂不处理
high_missing_cols = ['education_description', 'marriage_status_description', 'children_num_description']
merged_features[high_missing_cols] = merged_features[high_missing_cols].fillna('Unknown')  # 或保留为NaN
```

新增缺失值指示符：
为每个高缺失率特征添加二值标记（如 is_education_missing），明确记录数据缺失状态，供后续分析参考。
示例代码：

```python
for col in high_missing_cols:
    merged_features[f'is_{col}_missing'] = merged_features[col].isnull().astype(int)
```
业务逻辑优先级
教育程度：若业务假设认为学历可能影响骑手效率，可暂时保留；否则标记为未知。

婚姻与子女数：若平台有骑手稳定性相关需求（如长期留存），可暂时保留；否则标记为缺失。

针对interval_waybill和interval_work：缺失比例较小，可考虑直接用中位数填充。

3. 下一步，先对缺失比例较低的进行填充，覆盖原来的merged_data.csv
先创建另外一个脚本，把gender列男、女分别替换为1、0，weight_weather_level列换成对应的程度数字，比如正常=0，一般恶劣=1（我只是举个例子，需要针对情况进行具体排序），wind_grade_range拆分为两列（eg.0~4级）拆分为0和4，其他列暂时不动。
已完成：目前最新数据总表为`data\processed_data\transformed_merged_data.csv`

列 'interval_waybill' 处理结果:
- 总缺失率: 0.04%
- 缺失行数: 22
- 填充值(中位数): 4.9688889

列 'interval_work' 处理结果:
- 总缺失率: 0.04%
- 缺失行数: 22
- 填充值(中位数): 6.23638888333333

**目前最新数据总表为`data\processed_data\low_missing_filled_transformed_merged_data.csv`**
意思是、merge代表合并了三个表，low_missing_filled代表对缺失值进行了填充，transformed代表对某些列进行了转换。